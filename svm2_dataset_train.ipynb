{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06af8b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsvm_dataset_train.py\\n\\n\\nOne-click dataset builder + lightweight labeling + quick SVM trainer (Dash)\\nfor motion-state classification from PPG + IMU.\\n\\n\\nThis revision implements your requested changes:\\n1) Fix: import kurtosis/skew from scipy.stats (not scipy.signal).\\n2) Segment default range = full data duration.\\n3) Labels merged/split: Sit&Stand → one class; Transit and StrongMotion → two classes.\\n4) PSD preview x-range limited to 0-8 Hz.\\n5) Left control panel width doubled.\\n6) Training option (default ON): exclude Gyro & Jerk features; OFF = use all features.\\n7) Dataset source selector: in-memory (Store) by default, or pick a saved CSV from ./datasets.\\n8) In-memory dataset preview: first 3 rows + total count.\\n9) Spectral-shape features (entropy & main peak) added for IMU (AccMag, GyroMag, JerkMag) in addition to PPG.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "svm_dataset_train.py\n",
    "\n",
    "\n",
    "One-click dataset builder + lightweight labeling + quick SVM trainer (Dash)\n",
    "for motion-state classification from PPG + IMU.\n",
    "\n",
    "\n",
    "This revision implements your requested changes:\n",
    "1) Fix: import kurtosis/skew from scipy.stats (not scipy.signal).\n",
    "2) Segment default range = full data duration.\n",
    "3) Labels merged/split: Sit&Stand → one class; Transit and StrongMotion → two classes.\n",
    "4) PSD preview x-range limited to 0-8 Hz.\n",
    "5) Left control panel width doubled.\n",
    "6) Training option (default ON): exclude Gyro & Jerk features; OFF = use all features.\n",
    "7) Dataset source selector: in-memory (Store) by default, or pick a saved CSV from ./datasets.\n",
    "8) In-memory dataset preview: first 3 rows + total count.\n",
    "9) Spectral-shape features (entropy & main peak) added for IMU (AccMag, GyroMag, JerkMag) in addition to PPG.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06985d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfirst split holdout test data \\nthen moving window\\n\\n\\n21.09.2025\\nprevent data leakage\\n- Subject-wise split\\n- Pipeline-aware preprocessing\\n- Independent windowing\\n- Rigorous cross-validation\\n- External validation\\n- Transparent reporting\\n\\npipeline in script\\n- read csv\\n- label and sample by file\\n- preprocess by file\\n- split by sliding windows in each file\\n- feature in each window\\n- cv and holdout by files\\n- preview\\n\\ntime point of resample from label\\nadd x,y,z axis of imu features\\nadd external test and score\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first split holdout test data \n",
    "then moving window\n",
    "\n",
    "\n",
    "21.09.2025\n",
    "prevent data leakage\n",
    "- Subject-wise split\n",
    "- Pipeline-aware preprocessing\n",
    "- Independent windowing\n",
    "- Rigorous cross-validation\n",
    "- External validation\n",
    "- Transparent reporting\n",
    "\n",
    "pipeline in script\n",
    "- read csv\n",
    "- label and sample by file\n",
    "- preprocess by file\n",
    "- split by sliding windows in each file\n",
    "- feature in each window\n",
    "- cv and holdout by files\n",
    "- preview\n",
    "\n",
    "time point of resample from label\n",
    "add x,y,z axis of imu features\n",
    "add external test and score\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a743f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy.stats import kurtosis, skew \n",
    "\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, State, dash_table,no_update\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import (StratifiedKFold, GroupKFold, train_test_split,\n",
    "                                     GroupShuffleSplit, cross_validate)\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,           \n",
    "    balanced_accuracy_score,    \n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5210e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Global config\n",
    "# ========================\n",
    "FS_DEFAULT = 400  # Sampling frequency in Hz\n",
    "MIN_BPM = 40  # Minimum expected heart rate for artifact rejection\n",
    "MAX_BPM = 180  # Maximum expected heart rate for artifact rejection\n",
    "DATASET_DIR = Path(\"datasets\")     # Feature CSV snapshots\n",
    "MODEL_DIR = Path(\"models\")         # Saved models (pkl)\n",
    "PORT = 8051\n",
    "G = 9.81\n",
    "for p in (DATASET_DIR, MODEL_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733be9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "envi = 1\n",
    "\n",
    "windows_address_1 = [\"/mnt/d/Tubcloud/Shared/PPG/Test Data\",\n",
    "                   \"/mnt/d/Tubcloud/Shared/PPG/Test Data/25July25\"]\n",
    "\n",
    "ubuntu_address_0 = [\"/home/trinker/only_view/Test Data\", \n",
    "                  \"/home/trinker/only_view/Test Data/25July25\"]\n",
    "\n",
    "\n",
    "if envi:\n",
    "    DEFAULT_FOLDER_MAIN = windows_address_1[0]\n",
    "    DEFAULT_FOLDER = windows_address_1[1]\n",
    "else:\n",
    "    DEFAULT_FOLDER_MAIN = ubuntu_address_0[0]\n",
    "    DEFAULT_FOLDER = ubuntu_address_0[1]\n",
    "    \n",
    "\n",
    "folder_options = [\n",
    "    {\"label\": Path(p).as_posix(), \"value\": p}\n",
    "    for p in [DEFAULT_FOLDER_MAIN, DEFAULT_FOLDER]\n",
    "    if os.path.exists(p)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b513c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK = Path('.')\n",
    "DIR_TRAIN_LABELED = WORK/\"train_labeled\"   # file/path/t0/t1/label (timeline labels)\n",
    "DIR_TRAIN_RAW     = WORK/\"train_raw\"       # file-level raw segments (JSON vectors)\n",
    "DIR_TRAIN_WIN     = WORK/\"train_window\"    # windowed features\n",
    "DIR_TRAIN_VAL     = WORK/\"train_val\"\n",
    "MODEL_DIR         = WORK/\"models\"\n",
    "for p in (DIR_TRAIN_LABELED, DIR_TRAIN_RAW, DIR_TRAIN_WIN, MODEL_DIR, DIR_TRAIN_VAL):\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990fe606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import success\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Try to import your project core funcs\n",
    "# --------------------------------------\n",
    "try:\n",
    "    from funcs import preprocess_ppg_min, imu_preprocess_with_kf\n",
    "    print(\"import success\")\n",
    "    HAVE_CORE = True\n",
    "except Exception:\n",
    "    HAVE_CORE = False\n",
    "    print(\"import fail\")\n",
    "    def preprocess_ppg_min(ppg, fs=FS_DEFAULT, hp_cut=0.2, mains=None):\n",
    "        \"\"\"Fallback: high-pass 0.2 Hz + optional mains notch (50/60 Hz).\"\"\"\n",
    "        ppg = np.asarray(ppg, float).ravel()\n",
    "        b, a = signal.butter(2, hp_cut/(0.5*fs), 'high')\n",
    "        y = signal.filtfilt(b, a, ppg)\n",
    "        if mains in (50, 60):\n",
    "            b, a = signal.iirnotch(w0=float(mains), Q=30.0, fs=fs)\n",
    "            y = signal.filtfilt(b, a, y)\n",
    "        return y\n",
    "\n",
    "    def imu_preprocess_with_kf(df: pd.DataFrame, fs=FS_DEFAULT, acc_fc=20, gyro_fc=40, static_secs=2.0):\n",
    "        \"\"\"Fallback: LP accel/gyro → magnitudes + jerk. (No EKF here.)\"\"\"\n",
    "        G = 9.81\n",
    "        acc = df[['AX','AY','AZ']].to_numpy(float) * G\n",
    "        gyr = np.deg2rad(df[['GX','GY','GZ']].to_numpy(float))\n",
    "        def lp(x, fc):\n",
    "            b, a = signal.butter(4, fc/(0.5*fs), 'low')\n",
    "            return signal.filtfilt(b, a, x, axis=0)\n",
    "        acc_f = lp(acc, acc_fc)\n",
    "        gyr_f = lp(gyr, gyro_fc)\n",
    "        a_dyn = acc_f\n",
    "        acc_mag  = np.linalg.norm(a_dyn, axis=1)\n",
    "        gyro_mag = np.linalg.norm(gyr_f, axis=1)\n",
    "        jerk     = np.diff(a_dyn, axis=0, prepend=a_dyn[:1]) * fs\n",
    "        jerk_mag = np.linalg.norm(jerk, axis=1)\n",
    "        return dict(acc_f=acc_f, gyr_f=gyr_f, a_dyn=a_dyn,\n",
    "                    AccMag=acc_mag, GyroMag=gyro_mag, JerkMag=jerk_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb64d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Feature engineering\n",
    "# ========================\n",
    "\n",
    "\n",
    "\n",
    "def welch_bandpower(x, fs, fmin, fmax, nperseg=None, noverlap=0.5):\n",
    "    \"\"\"Band power via Welch (Hann). Returns a scalar.\"\"\"\n",
    "    x = np.asarray(x, float).ravel()\n",
    "    if nperseg is None:\n",
    "        nperseg = int(min(len(x), 2*fs))\n",
    "    nlap = int(noverlap * nperseg)\n",
    "    f, P = signal.welch(x, fs=fs, window=\"hann\", nperseg=nperseg, noverlap=nlap)\n",
    "    m = (f >= fmin) & (f <= fmax)\n",
    "    return float(np.trapz(P[m], f[m])) if np.any(m) else 0.0\n",
    "\n",
    "def spectral_entropy(x, fs, fmax=10.0):\n",
    "    \"\"\"Shannon entropy of normalized Welch PSD up to fmax.\"\"\"\n",
    "    x = np.asarray(x, float).ravel()\n",
    "    nperseg = int(min(len(x), 2*fs)) if len(x) else 256\n",
    "    f, P = signal.welch(x, fs=fs, window=\"hann\", nperseg=max(64, nperseg))\n",
    "    m = f <= fmax\n",
    "    p = P[m] + 1e-18\n",
    "    p /= np.sum(p)\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "\n",
    "def spectral_main_peak(x, fs, fmin=0.3, fmax=8.0):\n",
    "    \"\"\"Dominant frequency (Hz) within [fmin,fmax] from Welch PSD.\"\"\"\n",
    "    x = np.asarray(x, float).ravel()\n",
    "    if x.size < 8:\n",
    "        return 0.0\n",
    "    nperseg = int(min(len(x), 2*fs))\n",
    "    f, P = signal.welch(x, fs=fs, window=\"hann\", nperseg=max(64, nperseg))\n",
    "    band = (f >= fmin) & (f <= fmax)\n",
    "    return float(f[band][np.argmax(P[band])]) if np.any(band) else 0.0\n",
    "\n",
    "\n",
    "def compute_time_stats(x, prefix):\n",
    "    \"\"\"Basic time-domain stats; names carry a prefix (signal role).\"\"\"\n",
    "    x = np.asarray(x, float).ravel()\n",
    "    names = [f\"{prefix}_{k}\" for k in (\"mean\",\"std\",\"rms\",\"iqr\",\"kurt\",\"skew\")]\n",
    "    vals = [\n",
    "            float(np.mean(x)),\n",
    "            float(np.std(x)),\n",
    "            float(np.sqrt(np.mean(x**2))),\n",
    "            float(np.percentile(x, 75) - np.percentile(x, 25)),\n",
    "            float(kurtosis(x, fisher=False)), # Pearson definition (normal=3)\n",
    "            float(skew(x))\n",
    "            ]\n",
    "    return names, vals\n",
    "\n",
    "def extract_features_window_noattitude(ppg_seg: np.ndarray, imu_mag: Dict[str,np.ndarray], fs: float) -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Features per window. Includes PPG + IMU:\n",
    "    • Time stats: PPG, AccMag, GyroMag, JerkMag\n",
    "    • Bandpowers: 0.1–0.5 / 0.5–3 / 3–8 Hz for PPG, AccMag, GyroMag\n",
    "    • Spectral shape: spectral entropy + dominant peak for PPG, AccMag, GyroMag, JerkMag\n",
    "    \"\"\"\n",
    "    # Minimal PPG preprocessing to remove drift & mains\n",
    "    ppgm = preprocess_ppg_min(ppg_seg, fs=fs, hp_cut=0.2, mains=50)\n",
    "    acc_mag, gyro_mag, jerk_mag = imu_mag['AccMag'], imu_mag['GyroMag'], imu_mag['JerkMag']\n",
    "\n",
    "\n",
    "    feat_names, feat_vals = [], []\n",
    "    # Time-domain stats for each channel\n",
    "    for sig, pfx in [\n",
    "    (ppgm, \"ppg\"), (acc_mag, \"accmag\"), (gyro_mag, \"gyromag\"), (jerk_mag, \"jerkmag\")\n",
    "    ]:\n",
    "        n, v = compute_time_stats(sig, pfx)\n",
    "        feat_names += n; feat_vals += v\n",
    "\n",
    "\n",
    "    # Bandpowers for selected channels\n",
    "    bands = [(0.1,0.5),(0.5,3.0),(3.0,8.0)]\n",
    "    for lo, hi in bands:\n",
    "        feat_names += [f\"ppg_bp_{lo}-{hi}\", f\"acc_bp_{lo}-{hi}\", f\"gyro_bp_{lo}-{hi}\"]\n",
    "        feat_vals += [\n",
    "        welch_bandpower(ppgm, fs, lo, hi),\n",
    "        welch_bandpower(acc_mag, fs, lo, hi),\n",
    "        welch_bandpower(gyro_mag, fs, lo, hi)\n",
    "        ]\n",
    "\n",
    "\n",
    "    # Spectral-shape features for PPG + IMU (9): entropy & main peak freq\n",
    "    for sig, pfx in [\n",
    "    (ppgm, \"ppg\"), (acc_mag, \"accmag\"), (gyro_mag, \"gyromag\"), (jerk_mag, \"jerkmag\")\n",
    "    ]:\n",
    "        feat_names += [f\"{pfx}_spec_entropy\", f\"{pfx}_main_freq\"]\n",
    "        feat_vals += [spectral_entropy(sig, fs, fmax=10.0), spectral_main_peak(sig, fs, fmin=0.1, fmax=8.0)]\n",
    "\n",
    "\n",
    "    return feat_names, np.array(feat_vals, float)\n",
    "\n",
    "\n",
    "# Sliding window helper\n",
    "def make_windows(N: int, fs: float, win_sec: float, hop_sec: float):\n",
    "    W, H = int(win_sec*fs), int(hop_sec*fs)\n",
    "    for s in range(0, max(1, N-W+1), H):\n",
    "        yield s, s+W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b5c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Feature extractor (extended)\n",
    "# =========================\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "def _welch_psd(x, fs, fmax=8.0, nperseg=None, noverlap=None, detrend=\"constant\"):\n",
    "    \"\"\"Welch PSD bounded to [0, fmax]. No global state.\"\"\"\n",
    "    x = np.asarray(x, float).ravel()\n",
    "    if len(x) < 8:  # guard\n",
    "        return np.array([0.0]), np.array([0.0])\n",
    "    if nperseg is None:\n",
    "        nperseg = max(64, min(1024, int(len(x)//2)))\n",
    "    if noverlap is None:\n",
    "        noverlap = nperseg//2\n",
    "    f, Pxx = signal.welch(x, fs=fs, nperseg=nperseg, noverlap=noverlap,\n",
    "                          detrend=detrend, scaling=\"density\")\n",
    "    m = (f >= 0) & (f <= float(fmax))\n",
    "    return f[m], Pxx[m]\n",
    "\n",
    "def _band_power(f, Pxx, band, relative_to=(0.0, 8.0)):\n",
    "    \"\"\"Integrate power in band; optionally return relative power to reference band.\"\"\"\n",
    "    if len(f) == 0 or len(Pxx) == 0:\n",
    "        return np.nan\n",
    "    f = np.asarray(f); Pxx = np.asarray(Pxx)\n",
    "    df = np.diff(f); df = np.r_[df, df[-1]]\n",
    "    lo, hi = band\n",
    "    m = (f >= lo) & (f <= hi)\n",
    "    p = float(np.sum(Pxx[m]*df[m]))\n",
    "    if relative_to is None:\n",
    "        return p\n",
    "    rlo, rhi = relative_to\n",
    "    mr = (f >= rlo) & (f <= rhi)\n",
    "    pref = float(np.sum(Pxx[mr]*df[mr])) + 1e-12\n",
    "    return p / pref\n",
    "\n",
    "def _spec_entropy(Pxx):\n",
    "    \"\"\"Shannon entropy normalized to [0,1].\"\"\"\n",
    "    if len(Pxx) == 0:\n",
    "        return np.nan\n",
    "    P = np.maximum(np.asarray(Pxx, float), 1e-20)\n",
    "    p = P / P.sum()\n",
    "    H = -(p*np.log(p)).sum()\n",
    "    return float(H / np.log(len(p)))\n",
    "\n",
    "def _main_freq_in(f, Pxx, frange):\n",
    "    \"\"\"Argmax frequency restricted in frange.\"\"\"\n",
    "    if len(f) == 0: return np.nan\n",
    "    lo, hi = frange\n",
    "    m = (f >= lo) & (f <= hi)\n",
    "    if not np.any(m): return np.nan\n",
    "    idx = np.argmax(Pxx[m])\n",
    "    return float(f[m][idx])\n",
    "\n",
    "def _time_feats(x, prefix):\n",
    "    \"\"\"Window-only time stats; no dataset-level info.\"\"\"\n",
    "    x = np.asarray(x, float).ravel()\n",
    "    if len(x) == 0:\n",
    "        return {f\"{prefix}_mean\":np.nan, f\"{prefix}_std\":np.nan, f\"{prefix}_rms\":np.nan,\n",
    "                f\"{prefix}_iqr\":np.nan, f\"{prefix}_skew\":np.nan, f\"{prefix}_kurtosis\":np.nan,\n",
    "                f\"{prefix}_ptp\":np.nan}\n",
    "    q25, q75 = np.percentile(x, [25, 75])\n",
    "    return {\n",
    "        f\"{prefix}_mean\": float(np.mean(x)),\n",
    "        f\"{prefix}_std\":  float(np.std(x, ddof=1) if len(x)>1 else 0.0),\n",
    "        f\"{prefix}_rms\":  float(np.sqrt(np.mean(x**2))),\n",
    "        f\"{prefix}_iqr\":  float(q75 - q25),\n",
    "        f\"{prefix}_skew\": float(skew(x, bias=False)) if len(x) > 2 else 0.0,\n",
    "        f\"{prefix}_kurtosis\": float(kurtosis(x, fisher=False, bias=False)) if len(x) > 3 else 3.0,\n",
    "        f\"{prefix}_ptp\":  float(np.ptp(x)),\n",
    "    }\n",
    "\n",
    "def extract_features_window(\n",
    "    fs: float,\n",
    "    # PPG required (already minimally preprocessed for features, e.g., high-pass)\n",
    "    ppg_win: np.ndarray,\n",
    "    # IMU references (precomputed per-sample, same window slice)\n",
    "    a_dyn_xyz: tuple[np.ndarray,np.ndarray,np.ndarray] | None = None,  # dynamic accel X/Y/Z\n",
    "    accmag_win: np.ndarray | None = None,\n",
    "    gyromag_win: np.ndarray | None = None,\n",
    "    jerkmag_win: np.ndarray | None = None,\n",
    "    # Attitude series over the window (roll/pitch, radians)\n",
    "    roll_win: np.ndarray | None = None,\n",
    "    pitch_win: np.ndarray | None = None,\n",
    "    # Welch settings\n",
    "    psd_fmax: float = 8.0,\n",
    "    bp_bands: list[tuple[float,float]] = [(0.1,0.5),(0.5,3.0),(3.0,8.0)],\n",
    "    nperseg: int | None = None,\n",
    "    noverlap: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extended feature set including:\n",
    "      - PPG: time stats + PSD (0–8 Hz) band powers, spectral entropy, main_freq\n",
    "      - Magnitudes: AccMag/GyroMag/JerkMag (if provided): time + PSD features\n",
    "      - Axis-level dynamic acceleration: a_dyn_x/y/z (time + PSD features)\n",
    "      - Attitude: roll/pitch (time stats + their derivative stats)\n",
    "    All features are computed WINDOW-LOCAL and depend only on inputs; no globals.\n",
    "    Returns (names:list[str], values:np.ndarray[float]).\n",
    "    \"\"\"\n",
    "    names, vals = [], []\n",
    "\n",
    "    # ---- PPG features ----\n",
    "    tf = _time_feats(ppg_win, \"ppg\")\n",
    "    names += tf.keys(); vals += tf.values()\n",
    "    f, Pxx = _welch_psd(ppg_win, fs=fs, fmax=psd_fmax, nperseg=nperseg, noverlap=noverlap)\n",
    "    for lo,hi in bp_bands:\n",
    "        names.append(f\"ppg_bp_{lo:.1f}_{hi:.1f}\")\n",
    "        vals.append(_band_power(f, Pxx, (lo,hi), relative_to=(0.0, psd_fmax)))\n",
    "    names.append(\"ppg_spec_entropy\"); vals.append(_spec_entropy(Pxx))\n",
    "    names.append(\"ppg_main_freq\");    vals.append(_main_freq_in(f, Pxx, (0.3, 5.0)))  # HR-ish band\n",
    "\n",
    "    # ---- Magnitude features (if provided) ----\n",
    "    def _mag_block(x, prefix):\n",
    "        tfm = _time_feats(x, prefix); \n",
    "        nms, vls = list(tfm.keys()), list(tfm.values())\n",
    "        fi, Pi = _welch_psd(x, fs=fs, fmax=psd_fmax, nperseg=nperseg, noverlap=noverlap)\n",
    "        for lo,hi in bp_bands:\n",
    "            nms.append(f\"{prefix}_bp_{lo:.1f}_{hi:.1f}\")\n",
    "            vls.append(_band_power(fi, Pi, (lo,hi), relative_to=(0.0, psd_fmax)))\n",
    "        nms.append(f\"{prefix}_spec_entropy\"); vls.append(_spec_entropy(Pi))\n",
    "        nms.append(f\"{prefix}_main_freq\");    vls.append(_main_freq_in(fi, Pi, (0.0, psd_fmax)))\n",
    "        return nms, vls\n",
    "\n",
    "    if accmag_win is not None: \n",
    "        n,v = _mag_block(accmag_win, \"accmag\"); names+=n; vals+=v\n",
    "    if gyromag_win is not None: \n",
    "        n,v = _mag_block(gyromag_win, \"gyromag\"); names+=n; vals+=v\n",
    "    if jerkmag_win is not None: \n",
    "        n,v = _mag_block(jerkmag_win, \"jerkmag\"); names+=n; vals+=v\n",
    "\n",
    "    # ---- Axis-level dynamic accel features (directional) ----\n",
    "    if a_dyn_xyz is not None:\n",
    "        ax, ay, az = a_dyn_xyz\n",
    "        for arr, tag in [(ax,\"adynx\"), (ay,\"adyny\"), (az,\"adynz\")]:\n",
    "            tfa = _time_feats(arr, tag); names += tfa.keys(); vals += tfa.values()\n",
    "            fi, Pi = _welch_psd(arr, fs=fs, fmax=psd_fmax, nperseg=nperseg, noverlap=noverlap)\n",
    "            for lo,hi in bp_bands:\n",
    "                names.append(f\"{tag}_bp_{lo:.1f}_{hi:.1f}\")\n",
    "                vals.append(_band_power(fi, Pi, (lo,hi), relative_to=(0.0, psd_fmax)))\n",
    "            names.append(f\"{tag}_spec_entropy\"); vals.append(_spec_entropy(Pi))\n",
    "            names.append(f\"{tag}_main_freq\");    vals.append(_main_freq_in(fi, Pi, (0.0, psd_fmax)))\n",
    "\n",
    "    # ---- Attitude features (roll/pitch; and their rates) ----\n",
    "    def _att_block(theta, prefix):\n",
    "        tfatt = _time_feats(theta, prefix)\n",
    "        nms, vls = list(tfatt.keys()), list(tfatt.values())\n",
    "        # angular rate inside window (finite diff) – captures posture dynamics\n",
    "        d = np.diff(theta, prepend=theta[:1]) * fs\n",
    "        tfd = _time_feats(d, prefix + \"_dot\")\n",
    "        nms += tfd.keys(); vls += tfd.values()\n",
    "        return nms, vls\n",
    "\n",
    "    if roll_win is not None:\n",
    "        n,v = _att_block(np.asarray(roll_win, float), \"att_roll\"); names+=n; vals+=v\n",
    "    if pitch_win is not None:\n",
    "        n,v = _att_block(np.asarray(pitch_win, float), \"att_pitch\"); names+=n; vals+=v\n",
    "\n",
    "    return list(names), np.asarray(vals, float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c820b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Dash Func-------------------\n",
    "def get_folder_options():\n",
    "    \"\"\"遍历 DEFAULT_FOLDER_MAIN 下的子文件夹，生成 Dropdown 选项；确保包含 DEFAULT_FOLDER。\"\"\"\n",
    "    paths = []\n",
    "    if os.path.isdir(DEFAULT_FOLDER_MAIN):\n",
    "        for name in sorted(os.listdir(DEFAULT_FOLDER_MAIN)):\n",
    "            p = os.path.join(DEFAULT_FOLDER_MAIN, name)\n",
    "            if os.path.isdir(p):\n",
    "                paths.append(p)\n",
    "    # 确保 DEFAULT_FOLDER 在选项里（即使不在 DEFAULT_FOLDER_MAIN 下，也加入）\n",
    "    if DEFAULT_FOLDER and os.path.exists(DEFAULT_FOLDER) and DEFAULT_FOLDER not in paths:\n",
    "        paths.insert(0, DEFAULT_FOLDER)\n",
    "    # label 显示目录名，value 为完整路径\n",
    "    return [{'label': os.path.basename(p) or p, 'value': p} for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ee065e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Labels & colors\n",
    "# ========================\n",
    "LABEL_MAP = {\n",
    "0: \"Rest\",\n",
    "1: \"Sit/Stand\",\n",
    "2: \"Walk\",\n",
    "3: \"Transition\",\n",
    "4: \"StrongMotion\",\n",
    "}\n",
    "LABEL_OPTIONS = [{\"label\": f\"{k} - {v}\", \"value\": k} for k,v in LABEL_MAP.items()]\n",
    "LABEL_COLORS = {\n",
    "0: \"#2ecc71\", # Rest – green\n",
    "1: \"#3498db\", # Sit/Stand – blue\n",
    "2: \"#e67e22\", # Walk – orange\n",
    "3: \"#f1c40f\", # Transition – yellow\n",
    "4: \"#e74c3c\", # StrongMotion – red\n",
    "}\n",
    "\n",
    "def discrete_colorscale_from_map(map_k2hex: Dict[int,str]):\n",
    "    ks = sorted(map_k2hex.keys())\n",
    "    if not ks:\n",
    "        return \"Viridis\"\n",
    "    vmin, vmax = ks[0], ks[-1]\n",
    "    scale = []\n",
    "    for k in ks:\n",
    "        v = 0.0 if vmax==vmin else (k - vmin) / (vmax - vmin)\n",
    "        scale.append([v, map_k2hex[k]])\n",
    "        scale.append([min(v+1e-6,1.0), map_k2hex[k]])\n",
    "    return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc65c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gio: http://localhost:8051: Operation not supported\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Dash app layout\n",
    "# ========================\n",
    "external_stylesheets: List[str] = []\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "webbrowser.open(f\"http://localhost:{PORT}\")\n",
    "app.title = \"SVM Dataset Builder + Trainer (PPG+IMU)\"\n",
    "folder_options = get_folder_options()\n",
    "\n",
    "left_panel_style = {\"border\":\"1px solid #eee\",\"borderRadius\":\"8px\",\"padding\":\"10px\"}\n",
    "\n",
    "right_panel_style = {\"border\":\"1px solid #eee\",\"borderRadius\":\"8px\",\"padding\":\"10px\"}\n",
    "\n",
    "table_style_table = {\"maxHeight\":\"260px\",\"overflowY\":\"auto\",\"overflowX\":\"auto\",\"maxWidth\":\"100%\",\"minWidth\":\"100%\"}\n",
    "table_style_cell  = {\"minWidth\":\"110px\",\"width\":\"140px\",\"maxWidth\":\"240px\",\"whiteSpace\":\"normal\",\"textAlign\":\"left\"}\n",
    "\n",
    "app.layout = html.Div(style={\"backgroundColor\":\"white\",\"padding\":\"12px\"}, children=[\n",
    "    html.H2(\"PPG+IMU: label → train_labeled → train_raw → train_window → train\"),\n",
    "    html.Div(style={\"display\":\"grid\",\"gridTemplateColumns\":\"30% 1fr\",\"gap\":\"12px\"}, children=[\n",
    "\n",
    "        # ------------------ Left control panel ------------------\n",
    "        html.Div([\n",
    "            html.H4(\"1) File & Columns\"),\n",
    "            dcc.Dropdown(id=\"input-folder\", options=folder_options, value=DEFAULT_FOLDER,\n",
    "                         clearable=False, placeholder='Select data folder', style={'width':'100%'}),\n",
    "            dcc.Dropdown(id=\"ddl-files\", options=[], placeholder=\"Select a CSV file...\", style={\"marginTop\":6}),\n",
    "            html.Div([\n",
    "                dcc.Input(id=\"input-ppg-col\", type=\"text\", value=\"IR\", placeholder=\"PPG column (IR/RED/custom)\", style={\"width\":\"48%\"}),\n",
    "                dcc.Input(id=\"input-fs\", type=\"number\", value=FS_DEFAULT, step=1, placeholder=\"Fs (Hz)\", style={\"width\":\"48%\",\"float\":\"right\"})\n",
    "            ], style={\"marginTop\":6}),\n",
    "            html.Div(id=\"div-head-preview\", style={\"marginTop\":8}),\n",
    "\n",
    "            html.H4(\"2) LabelStore (raw timeline) → save to train_labeled\"),\n",
    "            dcc.RangeSlider(id=\"rs-seg\", min=0, max=10, step=0.5, value=[0,10], tooltip={\"always_visible\":True}),\n",
    "            html.Div(id=\"txt-seg\", style={\"marginTop\":4}),\n",
    "            # —— 新增：Subject ID（不影响现有控件） ——\n",
    "            html.Div([\n",
    "                html.Label(\"Subject ID\"),\n",
    "                dcc.Input(\n",
    "                    id=\"input-subject\",               # ✅ 新增ID\n",
    "                    type=\"text\",\n",
    "                    placeholder=\"e.g., S001\",\n",
    "                    debounce=True,\n",
    "                    style={\"width\": \"140px\"}\n",
    "                ),\n",
    "            ], style={\"marginTop\": 4}),\n",
    "            dcc.Dropdown(id=\"ddl-label\", options=LABEL_OPTIONS, value=0, style={\"marginTop\":6}),\n",
    "            html.Button(\"Add Labeled Segment\", id=\"btn-add-label\", n_clicks=0,\n",
    "                        style={\"width\":\"100%\",\"marginTop\":6,\"backgroundColor\":\"#2ecc71\",\"color\":\"white\"}),\n",
    "            html.Div(id=\"div-labelstore-preview\", style={\"marginTop\":6}),\n",
    "            html.Button(\"Save LabelStore → train_labeled CSV\", id=\"btn-save-labeled\", n_clicks=0, style={\"width\":\"100%\",\"marginTop\":6}),\n",
    "            html.Div(id=\"txt-save-labeled\", style={\"marginTop\":6}),\n",
    "\n",
    "            html.H4(\"3) Build train_raw from train_labeled (extract raw segments as JSON)\"),\n",
    "            html.Button(\"Scan train_labeled\", id=\"btn-scan-labeled\", n_clicks=0),\n",
    "            dcc.Dropdown(id=\"ddl-train-labeled\", options=[], placeholder=\"Select a train_labeled CSV\", style={\"marginTop\":6}),\n",
    "            html.Button(\"Materialize Raw Segments → train_raw\", id=\"btn-build-raw\", n_clicks=0, style={\"width\":\"100%\",\"marginTop\":6}),\n",
    "            html.Div(id=\"txt-build-raw\", style={\"marginTop\":6}),\n",
    "\n",
    "            html.H4(\"4) Merge train_raw datasets\"),\n",
    "            html.Button(\"Scan train_raw\", id=\"btn-scan-raw\", n_clicks=0),\n",
    "            dcc.Dropdown(id=\"ddl-raw-a\", options=[], placeholder=\"Select train_raw A\", style={\"marginTop\":6}),\n",
    "            dcc.Dropdown(id=\"ddl-raw-b\", options=[], placeholder=\"Select train_raw B\", style={\"marginTop\":6}),\n",
    "            html.Button(\"Merge A + B → new train_raw\", id=\"btn-merge-raw\", n_clicks=0, style={\"width\":\"100%\",\"marginTop\":6}),\n",
    "            html.Div(id=\"txt-merge-raw\", style={\"marginTop\":6}),\n",
    "\n",
    "            html.H4(\"5) train_raw → train_window (preproc + window + features)\"),\n",
    "            html.Button(\"Scan train_raw\", id=\"btn-scan-raw2\", n_clicks=0),\n",
    "            dcc.Dropdown(id=\"ddl-train-raw\", options=[], placeholder=\"Select a train_raw CSV\", style={\"marginTop\":6}),\n",
    "            html.Div([\n",
    "                    html.Label(\"Save target (train_window / train_val)\"),\n",
    "                    dcc.RadioItems(\n",
    "                        id=\"ddl-save-target\",                     # new ID\n",
    "                        options=[{\"label\": \"train_window\", \"value\": \"train_window\"},\n",
    "                                {\"label\": \"train_val (external holdout)\", \"value\": \"train_val\"}],\n",
    "                        value=\"train_window\",\n",
    "                        labelStyle={\"display\": \"block\"}\n",
    "                    ),\n",
    "                    #html.Div(id=\"txt-trainwin-save-status\", style={\"marginTop\": \"4px\", \"fontSize\": 12, \"color\": \"#555\"})  \n",
    "                ], style={\"marginTop\": \"8px\"}),\n",
    "            html.Div([\n",
    "                    html.Div([\n",
    "                        html.Div(\"win_sec — window length in seconds (feature window size)\", style={\"marginBottom\":4}),\n",
    "                        dcc.Input(id=\"input-win\", type=\"number\", value=3.0, step=0.5,\n",
    "                                placeholder=\"win_sec\", style={\"width\":\"100%\"})\n",
    "                    ], style={\"marginTop\":6}),\n",
    "                    html.Div([\n",
    "                        html.Div(\"hop_sec — hop/step in seconds (stride between windows)\", style={\"marginBottom\":4}),\n",
    "                        dcc.Input(id=\"input-hop\", type=\"number\", value=1.0, step=0.5,\n",
    "                                placeholder=\"hop_sec\", style={\"width\":\"100%\"})\n",
    "                    ], style={\"marginTop\":6}),\n",
    "                    html.Div([\n",
    "                        html.Div(\"min_overlap — reserved (unused here), keep default\", style={\"marginBottom\":4}),\n",
    "                        dcc.Input(id=\"input-minoverlap\", type=\"number\", value=0.7, step=0.05,\n",
    "                                placeholder=\"min_overlap (unused)\", style={\"width\":\"100%\"})\n",
    "                    ], style={\"marginTop\":6}),\n",
    "                ]),\n",
    "            html.Button(\"Build Windowed Features → train_window\", id=\"btn-build-win\", n_clicks=0, style={\"width\":\"100%\",\"marginTop\":6}),\n",
    "            html.Div(id=\"txt-build-win\", style={\"marginTop\":6}),\n",
    "\n",
    "            html.H4(\"6) Train from train_window\"),\n",
    "            html.Button(\"Scan train_window\", id=\"btn-scan-win\", n_clicks=0),\n",
    "            dcc.Dropdown(id=\"ddl-train-win\", options=[], placeholder=\"Select a train_window CSV\", style={\"marginTop\":6}),\n",
    "            html.Div(\"Holdout from /train_val (external only)\", style={\"marginTop\":6}),\n",
    "            html.Button(\"Refresh /train_val list\", id=\"btn-refresh-trainval\", n_clicks=0),   # newid\n",
    "            dcc.Dropdown(id=\"ddl-holdout-files\", options=[], value=[], multi=True,           # newid\n",
    "                        placeholder=\"Select CSV(s) in train_val for external holdout\",\n",
    "                        style={\"marginTop\":6}),\n",
    "            html.Div(\"Group by file (CV/holdout) — prevent leakage across same file\", style={\"marginTop\":6}),\n",
    "            dcc.Checklist(id=\"chk-group-file\",\n",
    "                        options=[{\"label\":\"Group by file (CV/holdout)\", \"value\":\"group\"}],\n",
    "                        value=[\"group\"], inline=True),\n",
    "\n",
    "            html.Div(\"Feature selection — exclude Gyro & Jerk features (default ON)\", style={\"marginTop\":6}),\n",
    "            dcc.Checklist(id=\"chk-excl-gj\",\n",
    "                        options=[{\"label\":\"Exclude Gyro & Jerk features (default ON)\", \"value\":\"excl\"}],\n",
    "                        value=[\"excl\"], inline=False, style={\"marginTop\":4}),\n",
    "            html.Div(\"Feature selection — Exclude Axis & Attitude features (default OFF)\", style={\"marginTop\":6}),\n",
    "            dcc.Checklist(id=\"chk-excl-axis\",\n",
    "                            options=[{\"label\": \"Exclude Axis & Attitude features (X/Y/Z + roll/pitch)\", \"value\": \"excl_axis\"}],\n",
    "                            value=[]), \n",
    "            html.Div(\"SVM kernel — RBF for non-linear boundaries; Linear for fast/linear\", style={\"marginTop\":6}),\n",
    "            dcc.RadioItems(id=\"ri-kernel\",\n",
    "               options=[{\"label\":\"RBF\",\"value\":\"rbf\"},{\"label\":\"Linear\",\"value\":\"linear\"}],\n",
    "               value=\"rbf\", inline=True),\n",
    "\n",
    "            html.Div([\n",
    "                html.Div(\"C — regularization strength (higher C = lower regularization)\", style={\"marginBottom\":4}),\n",
    "                dcc.Input(id=\"input-C\", type=\"number\", value=10.0, step=0.5,\n",
    "                        placeholder=\"C\", style={\"width\":\"100%\"})\n",
    "            ], style={\"marginTop\":6}),\n",
    "            html.Div([\n",
    "                html.Div(\"gamma — RBF kernel width (scale/auto/float)\", style={\"marginBottom\":4}),\n",
    "                dcc.Input(id=\"input-gamma\", type=\"text\", value=\"scale\",\n",
    "                        placeholder=\"gamma\", style={\"width\":\"100%\"})\n",
    "            ], style={\"marginTop\":6}),\n",
    "\n",
    "            html.Div(\"PCA (dimensionality reduction) — enable to reduce feature dimension\", style={\"marginTop\":6}),\n",
    "            dcc.Checklist(id=\"chk-pca\",\n",
    "                        options=[{\"label\":\"Use PCA (var)\", \"value\":\"use\"}],\n",
    "                        value=[\"use\"], inline=True),\n",
    "\n",
    "            html.Div(\"PCA retained variance — keep this proportion of variance (0.80–0.99)\", style={\"marginTop\":6}),\n",
    "            dcc.Slider(id=\"sl-pca-var\", min=0.80, max=0.99, step=0.01,\n",
    "                    value=0.95, marks=None, tooltip={\"always_visible\":True}),\n",
    "\n",
    "            html.Div([\n",
    "                html.Div(\"CV folds — number of cross-validation folds (grouped by file)\", style={\"marginBottom\":4}),\n",
    "                dcc.Input(id=\"input-cv\", type=\"number\", value=5, step=1,\n",
    "                        placeholder=\"CV folds\", style={\"width\":\"100%\"})\n",
    "            ], style={\"marginTop\":6}),\n",
    "            html.Div([\n",
    "                html.Div(\"Holdout ratio — validation split fraction (e.g., 0.2)\", style={\"marginBottom\":4}),\n",
    "                dcc.Input(id=\"input-test\", type=\"number\", value=0.2, step=0.05,\n",
    "                        placeholder=\"Holdout ratio\", style={\"width\":\"100%\"})\n",
    "            ], style={\"marginTop\":6}),\n",
    "\n",
    "            html.Button(\"Train Now\", id=\"btn-train\", n_clicks=0,\n",
    "                        style={\"width\":\"100%\",\"marginTop\":6,\"backgroundColor\":\"#34495e\",\"color\":\"white\"}),\n",
    "            html.Div(id=\"txt-train-status\", style={\"marginTop\":6,\"color\":\"#2c3e50\"}),\n",
    "        ], style=left_panel_style),\n",
    "\n",
    "        # ------------------ Right visualization panel ------------------\n",
    "        html.Div([\n",
    "            html.H4(\"Preview: Raw PPG & IMU (selected file / segment)\"),\n",
    "            dcc.Graph(id=\"fig-raw\", style={\"height\": \"520px\"}, config={\"responsive\": False}),\n",
    "\n",
    "            html.H4(\"Welch Spectra (PPG, AccMag, GyroMag)\"),\n",
    "            dcc.Graph(id=\"fig-psd\",  style={\"height\": \"320px\"}, config={\"responsive\": False}),\n",
    "\n",
    "            html.H4(\"Feature Table (train_window head)\"),\n",
    "            dash_table.DataTable(id=\"table-feats\", page_size=10,\n",
    "                                 style_table=table_style_table, style_cell=table_style_cell),\n",
    "            html.H4(\"Inference Preview on Selected Segment (predicted labels)\"),\n",
    "                dcc.Graph(id=\"fig-infer-preview\", style={\"height\": \"160px\"}, config={\"responsive\": False}),\n",
    "\n",
    "            html.H4(\"Training Quick Results\"),\n",
    "            html.Div(id=\"div-train-metrics\"),\n",
    "            html.Div(style={\"display\":\"grid\",\"gridTemplateColumns\":\"1fr 1fr\",\"gap\":\"8px\"}, children=[\n",
    "                dcc.Graph(id=\"fig-cm\", style={\"height\": \"360px\"}, config={\"responsive\": False}),\n",
    "                dcc.Graph(id=\"fig-f1\", style={\"height\": \"360px\"}, config={\"responsive\": False}),\n",
    "            ]),\n",
    "        ], style=right_panel_style),\n",
    "    ]),\n",
    "\n",
    "    # Stores\n",
    "    dcc.Store(id=\"store-folder\", data=DEFAULT_FOLDER),\n",
    "    dcc.Store(id=\"store-file-path\"),\n",
    "    dcc.Store(id=\"store-file-meta\"),\n",
    "    dcc.Store(id=\"store-labels\", data=[]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8414b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Callbacks: folder → files\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"ddl-files\", \"options\"),\n",
    "    Output(\"ddl-files\", \"value\"),\n",
    "    Output(\"store-folder\", \"data\"),\n",
    "    Input(\"input-folder\", \"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def update_file_list(folder):\n",
    "    if not folder or not os.path.exists(folder):\n",
    "        return [], None, folder\n",
    "    files = sorted([f for f in os.listdir(folder) if f.lower().endswith('.csv')])\n",
    "    opts = [{\"label\": Path(f).name, \"value\": os.path.join(folder, f)} for f in files]\n",
    "    default_val = os.path.join(folder, files[0]) if files else None\n",
    "    return opts, default_val, folder\n",
    "\n",
    "# ========================\n",
    "# Load & preview head\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"store-file-path\", \"data\"),\n",
    "    Output(\"div-head-preview\", \"children\"),\n",
    "    Output(\"store-file-meta\", \"data\"),\n",
    "    Output(\"rs-seg\", \"max\"),\n",
    "    Output(\"rs-seg\", \"value\"),\n",
    "    Input(\"ddl-files\", \"value\"),\n",
    "    State(\"input-fs\", \"value\"),\n",
    ")\n",
    "def load_file(file_path, fs):\n",
    "    if not file_path:\n",
    "        return None, html.Div(\"\"), None, 10, [0,10]\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        head = df.head(3)\n",
    "        table = dash_table.DataTable(\n",
    "            data=head.to_dict(\"records\"),\n",
    "            columns=[{\"name\": c, \"id\": c} for c in head.columns],\n",
    "            page_size=3,\n",
    "            style_table={\"overflowX\":\"auto\",\"maxWidth\":\"100%\"},\n",
    "            style_cell=table_style_cell\n",
    "        )\n",
    "        N = len(df); fs = float(fs or FS_DEFAULT)\n",
    "        dur = float(N/fs)\n",
    "        meta = {\"N\": N, \"duration\": dur}\n",
    "        new_max = max(5.0, round(dur, 2))\n",
    "        return file_path, table, meta, new_max, [0.0, new_max]\n",
    "    except Exception as e:\n",
    "        return None, html.Div(f\"Failed to load: {e}\"), None, 10, [0,10]\n",
    "\n",
    "@app.callback(Output(\"txt-seg\", \"children\"), Input(\"rs-seg\", \"value\"))\n",
    "def seg_text(value):\n",
    "    if not value:\n",
    "        return \"No segment selected.\"\n",
    "    return f\"Segment: {value[0]:.2f} s → {value[1]:.2f} s\"\n",
    "\n",
    "# ========================\n",
    "# LabelStore add & save to train_labeled\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"store-labels\", \"data\"),\n",
    "    Output(\"div-labelstore-preview\", \"children\"),\n",
    "    Input(\"btn-add-label\", \"n_clicks\"),\n",
    "    State(\"ddl-files\", \"value\"),\n",
    "    State(\"rs-seg\", \"value\"),\n",
    "    State(\"ddl-label\", \"value\"),\n",
    "    State(\"store-labels\", \"data\"),\n",
    "    State(\"input-subject\", \"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def add_labeled_interval(nc, file_path, seg, label_id, store_labels, subject_id):\n",
    "    store_labels = store_labels or []\n",
    "    if not file_path or not seg:\n",
    "        return store_labels, html.Div(\"Select file and segment first.\")\n",
    "    t0, t1 = float(seg[0]), float(seg[1])\n",
    "    if t1 <= t0:\n",
    "        return store_labels, html.Div(\"Invalid segment: end ≤ start.\")\n",
    "    entry = dict(\n",
    "        file=os.path.basename(file_path), \n",
    "        path=file_path, \n",
    "        t0=t0, \n",
    "        t1=t1, \n",
    "        label=int(label_id),\n",
    "        subject=(subject_id or \"\"))\n",
    "    store_labels.append(entry)\n",
    "    df_prev = pd.DataFrame(store_labels[-12:])\n",
    "    table = dash_table.DataTable(data=df_prev.to_dict(\"records\"),\n",
    "                                 columns=[{\"name\":c, \"id\":c} for c in df_prev.columns],\n",
    "                                 page_size=12, style_table=table_style_table, style_cell=table_style_cell)\n",
    "    return store_labels, html.Div([html.Div(f\"LabelStore size: {len(store_labels)}\"), table])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"txt-save-labeled\", \"children\"),\n",
    "    Input(\"btn-save-labeled\", \"n_clicks\"),\n",
    "    State(\"store-labels\", \"data\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def save_labelstore(nc, labels_data):\n",
    "    if not labels_data:\n",
    "        return \"LabelStore is empty.\"\n",
    "    df = pd.DataFrame(labels_data)\n",
    "    n_files = df['file'].nunique()\n",
    "    counts = df['label'].value_counts().to_dict()\n",
    "    label_part = \"_\".join([f\"L{k}-{counts.get(k,0)}\" for k in sorted(LABEL_MAP.keys())])\n",
    "    tag = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = DIR_TRAIN_LABELED / f\"labeled_F{n_files}_{label_part}_{tag}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    return f\"Saved train_labeled: {out.name} | rows={len(df)} | files={n_files}\"\n",
    "\n",
    "# ========================\n",
    "# train_labeled → train_raw (extract raw segments as JSON rows)\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"ddl-train-labeled\", \"options\"),\n",
    "    Input(\"btn-scan-labeled\", \"n_clicks\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def scan_labeled(_):\n",
    "    files = sorted(DIR_TRAIN_LABELED.glob(\"*.csv\"))\n",
    "    return [{\"label\": f.name, \"value\": str(f)} for f in files]\n",
    "\n",
    "# --- REPLACE the old build_train_raw() callback with this long-form version ---\n",
    "@app.callback(\n",
    "    Output(\"txt-build-raw\", \"children\"),\n",
    "    Input(\"btn-build-raw\", \"n_clicks\"),\n",
    "    State(\"ddl-train-labeled\", \"value\"),\n",
    "    State(\"input-ppg-col\", \"value\"),\n",
    "    State(\"input-fs\", \"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def build_train_raw_long(nc, labeled_csv, ppg_col, fs):\n",
    "    if not labeled_csv:\n",
    "        return \"Select a train_labeled CSV first.\"\n",
    "    fs = float(fs or FS_DEFAULT)\n",
    "    df_lab = pd.read_csv(labeled_csv)\n",
    "\n",
    "    out_rows = []   # 我们先收集，再一次性 concat，提高效率\n",
    "    files = df_lab['path'].unique().tolist()\n",
    "    seg_counter = 0\n",
    "\n",
    "    for fpath in files:\n",
    "        sub = df_lab[df_lab['path'] == fpath].reset_index(drop=True)\n",
    "        try:\n",
    "            df = pd.read_csv(fpath)\n",
    "        except Exception as e:\n",
    "            return f\"Failed to read {Path(fpath).name}: {e}\"\n",
    "\n",
    "        # 选择PPG列\n",
    "        ppg_col_use = ppg_col if ppg_col in df.columns else (\"IR\" if \"IR\" in df.columns else df.columns[0])\n",
    "\n",
    "        for _, r in sub.iterrows():\n",
    "            s = int(max(0, r.t0 * fs))\n",
    "            e = int(min(len(df), r.t1 * fs))\n",
    "            if e - s < int(0.5 * fs):\n",
    "                continue\n",
    "\n",
    "            # 切片\n",
    "            ppg = df[ppg_col_use].to_numpy(float)[s:e]\n",
    "            has_acc = set(['AX','AY','AZ']).issubset(df.columns)\n",
    "            has_gyr = set(['GX','GY','GZ']).issubset(df.columns)\n",
    "\n",
    "            if has_acc:\n",
    "                AX = df['AX'].to_numpy(float)[s:e]\n",
    "                AY = df['AY'].to_numpy(float)[s:e]\n",
    "                AZ = df['AZ'].to_numpy(float)[s:e]\n",
    "            else:\n",
    "                AX = AY = AZ = None\n",
    "\n",
    "            if has_gyr:\n",
    "                GX = df['GX'].to_numpy(float)[s:e]\n",
    "                GY = df['GY'].to_numpy(float)[s:e]\n",
    "                GZ = df['GZ'].to_numpy(float)[s:e]\n",
    "            else:\n",
    "                GX = GY = GZ = None\n",
    "\n",
    "            # 逐样本长表\n",
    "            N = len(ppg)\n",
    "            t = np.arange(N, dtype=float) / fs\n",
    "            base = {\n",
    "                'file': os.path.basename(fpath),\n",
    "                'seg_id': seg_counter,\n",
    "                'label': int(r.label),\n",
    "                'fs': fs,\n",
    "                'ppg_col': ppg_col_use,\n",
    "            }\n",
    "            df_seg = pd.DataFrame({\n",
    "                **base,\n",
    "                't': t,\n",
    "                'PPG': ppg\n",
    "            })\n",
    "            if AX is not None:\n",
    "                df_seg['AX'] = AX; df_seg['AY'] = AY; df_seg['AZ'] = AZ\n",
    "            if GX is not None:\n",
    "                df_seg['GX'] = GX; df_seg['GY'] = GY; df_seg['GZ'] = GZ\n",
    "\n",
    "            out_rows.append(df_seg)\n",
    "            seg_counter += 1\n",
    "\n",
    "    if not out_rows:\n",
    "        return \"No segments produced. Check labels/time ranges.\"\n",
    "\n",
    "    out_df = pd.concat(out_rows, ignore_index=True)\n",
    "\n",
    "    # 文件名包含文件数、各类样本数、时间戳（这里按“行数统计”不太直观，可按 seg_id 再做片段级计数）\n",
    "    n_files = out_df['file'].nunique()\n",
    "    counts = out_df.groupby('label')['t'].count().to_dict()  # 样本点数量统计\n",
    "    label_part = \"_\".join([f\"L{k}-{counts.get(k,0)}\" for k in sorted(LABEL_MAP.keys())])\n",
    "    tag = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = DIR_TRAIN_RAW / f\"trainrawLONG_F{n_files}_{label_part}_{tag}.csv\"\n",
    "\n",
    "    # ⚠️ CSV 可能很大，推荐改 parquet：out.with_suffix('.parquet')\n",
    "    out_df.to_csv(out, index=False)\n",
    "    return f\"Saved long-form train_raw: {out.name} | rows={len(out_df)} | files={n_files} | segments={out_df['seg_id'].nunique()}\"\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Merge train_raw A+B → new train_raw\n",
    "# ========================\n",
    "@app.callback(Output(\"ddl-raw-a\", \"options\"), Output(\"ddl-raw-b\", \"options\"), Input(\"btn-scan-raw\", \"n_clicks\"), prevent_initial_call=True)\n",
    "def scan_raw(_):\n",
    "    files = sorted(DIR_TRAIN_RAW.glob(\"*.csv\"))\n",
    "    opts = [{\"label\": f.name, \"value\": str(f)} for f in files]\n",
    "    return opts, opts\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"txt-merge-raw\", \"children\"),\n",
    "    Input(\"btn-merge-raw\", \"n_clicks\"),\n",
    "    State(\"ddl-raw-a\", \"value\"),\n",
    "    State(\"ddl-raw-b\", \"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def merge_raw(nc, A, B):\n",
    "    if not A or not B:\n",
    "        return \"Select two train_raw files.\"\n",
    "\n",
    "    try:\n",
    "        dfA, dfB = pd.read_csv(A), pd.read_csv(B)\n",
    "\n",
    "        # ---------- Detect schema ----------\n",
    "        is_long_A = {'file','label','seg_id','t','PPG'}.issubset(dfA.columns)\n",
    "        is_long_B = {'file','label','seg_id','t','PPG'}.issubset(dfB.columns)\n",
    "        is_long = is_long_A and is_long_B\n",
    "\n",
    "        if is_long:\n",
    "            # ===== Long-form merge =====\n",
    "            # dtype normalization\n",
    "            for col in ['file','seg_id','label','t','PPG']:\n",
    "                if col in dfA.columns:\n",
    "                    if col in ('seg_id','label'):\n",
    "                        dfA[col] = dfA[col].astype(int)\n",
    "                    elif col == 't' or col == 'PPG':\n",
    "                        dfA[col] = dfA[col].astype(float)\n",
    "                if col in dfB.columns:\n",
    "                    if col in ('seg_id','label'):\n",
    "                        dfB[col] = dfB[col].astype(int)\n",
    "                    elif col == 't' or col == 'PPG':\n",
    "                        dfB[col] = dfB[col].astype(float)\n",
    "\n",
    "            # 对齐列\n",
    "            cols = sorted(set(dfA.columns) | set(dfB.columns))\n",
    "            dfA = dfA.reindex(columns=cols)\n",
    "            dfB = dfB.reindex(columns=cols)\n",
    "\n",
    "            # ---- 关键：避免 seg_id 冲突（对 B 集）----\n",
    "            # 规则：对于每个 file，B 的 seg_id += (A 中该 file 的 max(seg_id) + 1)\n",
    "            if 'seg_id' in cols and 'file' in cols:\n",
    "                maxA = dfA.groupby('file')['seg_id'].max() if not dfA.empty else pd.Series(dtype=float)\n",
    "                # 给不存在于 A 的 file 也留 0 偏移\n",
    "                for f in dfB['file'].unique():\n",
    "                    off = (int(maxA.get(f, -1)) + 1) if not np.isnan(maxA.get(f, np.nan)) else 0\n",
    "                    m = (dfB['file'] == f)\n",
    "                    dfB.loc[m, 'seg_id'] = dfB.loc[m, 'seg_id'].astype(int) + off\n",
    "\n",
    "            # 合并 + 去重\n",
    "            df = pd.concat([dfA, dfB], ignore_index=True)\n",
    "            # 依据长表主键去重（file, seg_id, t）——避免重复拼接\n",
    "            key_cols = [c for c in ['file','seg_id','t'] if c in df.columns]\n",
    "            if key_cols:\n",
    "                df = df.drop_duplicates(subset=key_cols)\n",
    "\n",
    "            # 统计 & 命名\n",
    "            n_files = df['file'].nunique()\n",
    "            counts = df['label'].value_counts().to_dict() if 'label' in df.columns else {}\n",
    "            label_part = \"_\".join([f\"L{k}-{counts.get(k,0)}\" for k in sorted(counts.keys())]) if counts else \"LNA\"\n",
    "            tag = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            out = DIR_TRAIN_RAW / f\"trainrawLONG_MERGED_F{n_files}_{label_part}_{tag}.csv\"\n",
    "            df.to_csv(out, index=False)\n",
    "\n",
    "            n_rows = len(df)\n",
    "            n_segs = df['seg_id'].nunique() if 'seg_id' in df.columns else 'NA'\n",
    "            return f\"Merged (long-form) → {out.name} | rows={n_rows} | files={n_files} | segs={n_segs}\"\n",
    "\n",
    "        else:\n",
    "            # ===== JSON-raw merge（你现有 schema）=====\n",
    "            cols = sorted(set(dfA.columns) | set(dfB.columns))\n",
    "            df = pd.concat([dfA.reindex(columns=cols), dfB.reindex(columns=cols)], ignore_index=True)\n",
    "            # 对 JSON-raw 通常无需去重；若担心重复，可按 (file, label, ppg_json) 去重：\n",
    "            if {'file','label','ppg_json'}.issubset(df.columns):\n",
    "                df = df.drop_duplicates(subset=['file','label','ppg_json'])\n",
    "\n",
    "            n_files = df['file'].nunique() if 'file' in df.columns else 0\n",
    "            counts = df['label'].value_counts().to_dict() if 'label' in df.columns else {}\n",
    "            label_part = \"_\".join([f\"L{k}-{counts.get(k,0)}\" for k in sorted(counts.keys())]) if counts else \"LNA\"\n",
    "            tag = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            out = DIR_TRAIN_RAW / f\"trainrawMERGED_F{n_files}_{label_part}_{tag}.csv\"\n",
    "            df.to_csv(out, index=False)\n",
    "            return f\"Merged (json-raw) → {out.name} | rows={len(df)} | files={n_files}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Merge failed: {e}\"\n",
    "\n",
    "# ========================\n",
    "# train_raw → train_window (per file preprocess + window + features)\n",
    "# ========================\n",
    "@app.callback(Output(\"ddl-train-raw\", \"options\"), Input(\"btn-scan-raw2\", \"n_clicks\"), prevent_initial_call=True)\n",
    "def scan_raw2(_):\n",
    "    files = sorted(DIR_TRAIN_RAW.glob(\"*.csv\"))\n",
    "    return [{\"label\": f.name, \"value\": str(f)} for f in files]\n",
    "\n",
    "# --- REPLACE the old build_train_window() with this version for long-form train_raw ---\n",
    "@app.callback(\n",
    "    Output(\"txt-build-win\", \"children\"),\n",
    "    Output(\"table-feats\", \"columns\", allow_duplicate=True),\n",
    "    Output(\"table-feats\", \"data\", allow_duplicate=True),\n",
    "    Input(\"btn-build-win\", \"n_clicks\"),\n",
    "    State(\"ddl-train-raw\", \"value\"),\n",
    "    State(\"input-win\", \"value\"),\n",
    "    State(\"input-hop\", \"value\"),\n",
    "    State(\"ddl-save-target\", \"value\"), \n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def build_train_window_from_raw_long(nc, raw_csv, win_sec, hop_sec, save_target, noattitude=None):\n",
    "    if not raw_csv:\n",
    "        return \"Select a train_raw CSV first.\", no_update, no_update\n",
    "\n",
    "    # 读取长表\n",
    "    dfR = pd.read_csv(raw_csv)\n",
    "    need_cols = {'file','seg_id','label','fs','t','PPG'}\n",
    "    if not need_cols.issubset(dfR.columns):\n",
    "        return \"train_raw schema mismatch: require columns file, seg_id, label, fs, t, PPG\", no_update, no_update\n",
    "\n",
    "    all_rows, feat_names = [], None\n",
    "\n",
    "    # 逐 (file, seg_id) 重建片段 → 预处理IMU → 滑窗 → 特征\n",
    "    for (file_i, seg_id), grp in dfR.groupby(['file','seg_id']):\n",
    "        grp = grp.sort_values('t')\n",
    "        fs = float(grp['fs'].iloc[0])\n",
    "        label = int(grp['label'].iloc[0])\n",
    "\n",
    "        ppg = grp['PPG'].to_numpy(float)\n",
    "        N = len(ppg)\n",
    "\n",
    "        # IMU 列可能不存在\n",
    "        AX = grp['AX'].to_numpy(float) #if 'AX' in grp.columns else np.zeros(N)\n",
    "        AY = grp['AY'].to_numpy(float)#if 'AY' in grp.columns else np.zeros(N)\n",
    "        AZ = grp['AZ'].to_numpy(float) #if 'AZ' in grp.columns else np.zeros(N)\n",
    "        GX = grp['GX'].to_numpy(float) #if 'GX' in grp.columns else np.zeros(N)\n",
    "        GY = grp['GY'].to_numpy(float) #if 'GY' in grp.columns else np.zeros(N)\n",
    "        GZ = grp['GZ'].to_numpy(float) #if 'GZ' in grp.columns else np.zeros(N)\n",
    "\n",
    "        df_seg = pd.DataFrame({'AX':AX,'AY':AY,'AZ':AZ,'GX':GX,'GY':GY,'GZ':GZ})\n",
    "        \n",
    "        imu_out = imu_preprocess_with_kf(df_seg, fs=fs)\n",
    "        acc_g = imu_out['acc_raw']\n",
    "        gyro_d = imu_out['gyro_raw']\n",
    "        acc = imu_out['acc']\n",
    "        gyro = imu_out['gyro']\n",
    "        acc_lp = imu_out['acc_f'] \n",
    "        gyro_lp = imu_out['gyr_f'] \n",
    "        jerk = imu_out['jerk'] \n",
    "        roll=imu_out['roll'] \n",
    "        pitch=imu_out['pitch'] \n",
    "        bg = imu_out['gyro_bias']\n",
    "        g_body=imu_out['g_body']\n",
    "        g_dir = imu_out['g_dir'] \n",
    "        a_dyn= imu_out['a_dyn']\n",
    "        acc_mag = imu_out['AccMag'] \n",
    "        gyro_mag = imu_out['GyroMag'] \n",
    "        jerk_mag = imu_out['JerkMag']\n",
    "\n",
    "        for s, e in make_windows(N, fs, float(win_sec), float(hop_sec)):\n",
    "            if e - s < int(0.5 * fs):\n",
    "                continue\n",
    "            ppg_seg = ppg[s:e]\n",
    "            if noattitude:\n",
    "                \n",
    "                imu_seg = {\n",
    "                    'AccMag': acc_mag[s:e],\n",
    "                    'GyroMag': gyro_mag[s:e],\n",
    "                    'JerkMag': jerk_mag[s:e],\n",
    "                }\n",
    "                feat_names, feats = extract_features_window_noattitude(ppg_seg, imu_seg, fs)\n",
    "            else:    \n",
    "                ppg_win = preprocess_ppg_min(ppg_seg, fs=fs, hp_cut=0.2, mains=50)\n",
    "                ax_win = a_dyn[s:e, 0]; ay_win = a_dyn[s:e, 1]; az_win = a_dyn[s:e, 2]\n",
    "                accmag_win = acc_mag[s:e]\n",
    "                gyromag_win = gyro_mag[s:e] #if 'GyroMag' in locals() else None\n",
    "                jerkmag_win = jerk_mag[s:e] #if 'JerkMag' in locals() else None\n",
    "                roll_win = roll[s:e]\n",
    "                pitch_win = pitch[s:e]\n",
    "                feat_names, feats = extract_features_window(\n",
    "                    fs=fs,\n",
    "                    ppg_win=ppg_win,\n",
    "                    a_dyn_xyz=(ax_win, ay_win, az_win),\n",
    "                    accmag_win=accmag_win,\n",
    "                    gyromag_win=gyromag_win,\n",
    "                    jerkmag_win=jerkmag_win,\n",
    "                    roll_win=roll_win, pitch_win=pitch_win,\n",
    "                    psd_fmax=8.0\n",
    "                )\n",
    "            # 仍然保留 window 的绝对时间，方便对齐（使用片段内起止时间）\n",
    "            t0 = grp['t'].iloc[s]; t1 = grp['t'].iloc[e-1]\n",
    "            all_rows.append(feats.tolist() + [label, t0, t1, file_i])\n",
    "\n",
    "    if not all_rows:\n",
    "        return \"No windows generated — adjust win/hop.\", no_update, no_update\n",
    "\n",
    "    cols = feat_names + [\"label\",\"t_start\",\"t_end\",\"file\"]\n",
    "    ds_df = pd.DataFrame(all_rows, columns=cols)\n",
    "\n",
    "    # 保存 train_window\n",
    "    n_files = ds_df['file'].nunique()\n",
    "    counts = ds_df['label'].value_counts().to_dict()\n",
    "    label_part = \"_\".join([f\"L{k}-{counts.get(k,0)}\" for k in sorted(LABEL_MAP.keys())])\n",
    "    tag = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    save_dir = DIR_TRAIN_WIN if save_target != \"train_val\" else DIR_TRAIN_VAL \n",
    "    fname = f\"trainwin_F{n_files}_ws{float(win_sec):.2f}s_hs{float(hop_sec):.2f}s_{label_part}_{tag}.csv\" \n",
    "    out = save_dir / fname  \n",
    "    ds_df.to_csv(out, index=False)\n",
    "\n",
    "    head = ds_df.head(12)\n",
    "    cols_dash = [{\"name\": c, \"id\": c} for c in head.columns]\n",
    "    return f\"Saved to {save_dir.name}: {out.name} | rows={len(ds_df)} | files={n_files}\", cols_dash, head.to_dict(\"records\")\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Preview: raw signals & PSD (unchanged)\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"fig-raw\", \"figure\"),\n",
    "    Output(\"fig-psd\", \"figure\"),\n",
    "    Input(\"ddl-files\", \"value\"),\n",
    "    Input(\"rs-seg\", \"value\"),\n",
    "    State(\"input-ppg-col\", \"value\"),\n",
    "    State(\"input-fs\", \"value\"),\n",
    ")\n",
    "def preview_signals(file_path, seg, ppg_col, fs):\n",
    "    fig_empty = go.Figure().update_layout(height=240, paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "    if not file_path or not seg:\n",
    "        return fig_empty, fig_empty\n",
    "    fs = float(fs or FS_DEFAULT)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if ppg_col not in df.columns:\n",
    "            ppg_col = \"IR\" if \"IR\" in df.columns else df.columns[0]\n",
    "        t = np.arange(len(df)) / fs\n",
    "        m = (t >= seg[0]) & (t <= seg[1])\n",
    "        ppg = df[ppg_col].to_numpy(float)\n",
    "        imu = imu_preprocess_with_kf(df, fs=fs)\n",
    "        acc_mag, gyro_mag = imu['AccMag'], imu['GyroMag']\n",
    "\n",
    "        fig_t = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.04,\n",
    "                              subplot_titles=(f\"PPG ({ppg_col})\", \"AccMag (m/s²)\", \"GyroMag (rad/s)\"))\n",
    "        fig_t.add_trace(go.Scatter(x=t[m], y=ppg[m], name=\"PPG\", line=dict(color=\"purple\")), row=1, col=1)\n",
    "        fig_t.add_trace(go.Scatter(x=t[m], y=acc_mag[m], name=\"AccMag\", line=dict(color=\"crimson\")), row=2, col=1)\n",
    "        fig_t.add_trace(go.Scatter(x=t[m], y=gyro_mag[m], name=\"GyroMag\", line=dict(color=\"royalblue\")), row=3, col=1)\n",
    "        fig_t.update_layout(height=520, margin=dict(l=50,r=30,t=60,b=40), paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "        for r in (1,2,3):\n",
    "            fig_t.update_yaxes(showgrid=True, gridcolor=\"rgba(0,0,0,0.08)\", row=r, col=1)\n",
    "        fig_t.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "\n",
    "        def psd_curve(x, fs):\n",
    "            x = x[m]\n",
    "            if len(x) < int(2*fs):\n",
    "                nperseg = max(128, int(len(x)//2))\n",
    "            else:\n",
    "                nperseg = int(2*fs)\n",
    "            f, P = signal.welch(x, fs=fs, window=\"hann\", nperseg=nperseg, noverlap=int(0.5*nperseg))\n",
    "            return f, P\n",
    "        f1,P1 = psd_curve(ppg, fs)\n",
    "        f2,P2 = psd_curve(acc_mag, fs)\n",
    "        f3,P3 = psd_curve(gyro_mag, fs)\n",
    "\n",
    "        fig_f = go.Figure()\n",
    "        fig_f.add_trace(go.Scatter(x=f1, y=P1, name=\"PPG\", line=dict(color=\"purple\")))\n",
    "        fig_f.add_trace(go.Scatter(x=f2, y=P2, name=\"AccMag\", line=dict(color=\"crimson\")))\n",
    "        fig_f.add_trace(go.Scatter(x=f3, y=P3, name=\"GyroMag\", line=dict(color=\"royalblue\")))\n",
    "        fig_f.update_layout(height=320, margin=dict(l=50,r=30,t=50,b=40), paper_bgcolor=\"white\", plot_bgcolor=\"white\",\n",
    "                            xaxis_title=\"Frequency (Hz)\", yaxis_title=\"PSD\", xaxis=dict(range=[0,8]))\n",
    "        fig_f.update_xaxes(showgrid=True, gridcolor=\"rgba(0,0,0,0.08)\")\n",
    "        fig_f.update_yaxes(showgrid=True, gridcolor=\"rgba(0,0,0,0.08)\")\n",
    "        return fig_t, fig_f\n",
    "    except Exception as e:\n",
    "        fig = go.Figure().update_layout(title=f\"Error: {e}\", height=240, paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "        return fig, fig\n",
    "\n",
    "# ========================\n",
    "# Training from train_window\n",
    "# ========================\n",
    "\n",
    "def build_Xy_from_df(df: pd.DataFrame):\n",
    "    meta_cols = {\"label\",\"t_start\",\"t_end\",\"file\"}\n",
    "    feat_cols = [c for c in df.columns if c not in meta_cols]\n",
    "    X = df[feat_cols].to_numpy(float)\n",
    "    y = df[\"label\"].to_numpy(int)\n",
    "    groups = df[\"file\"].to_numpy(object) if \"file\" in df.columns else None\n",
    "    return X, y, groups, feat_cols\n",
    "\n",
    "def select_feat_cols(feat_cols: List[str], exclude_gj: bool, exclude_axis: bool) -> List[str]:\n",
    "    keep = feat_cols[:]\n",
    "    print(\"GY:\", exclude_gj)\n",
    "    if exclude_gj:\n",
    "        drop_prefixes = (\"gyromag_\", \"gyro_bp_\", \"gyromag_spec_entropy\", \"gyromag_main_freq\",\n",
    "                        \"jerkmag_\", \"jerk_bp_\", \"jerkmag_spec_entropy\", \"jerkmag_main_freq\")\n",
    "        keep = [c for c in feat_cols if not c.startswith(drop_prefixes)]\n",
    "    if exclude_axis:\n",
    "        drop_axis_prefix = (\"adynx_\", \"adyny_\", \"adynz_\", \"att_roll_\", \"att_pitch_\")\n",
    "        keep = [c for c in keep if not c.startswith(drop_axis_prefix)]\n",
    "    return keep\n",
    "\n",
    "def train_pipeline_quick(df_win: pd.DataFrame, kernel: str, C_val: float, gamma_val: str|float,\n",
    "                         use_pca: bool, pca_var: float, cv_folds: int, test_ratio: float,\n",
    "                         group_by_file: bool, exclude_gj: bool, exclude_axis: bool, random_state: int = 42):\n",
    "    \"\"\"Leak-safe training:\n",
    "       - Prefer grouped CV/holdout by file; fallback to seg_uid; else time-ordered split with a gap.\n",
    "       - Feature selection switch: exclude Gyro/Jerk if requested.\n",
    "    \"\"\"\n",
    "    # ------- features & labels -------\n",
    "    X_all, y, groups_file, feat_cols_all = build_Xy_from_df(df_win)\n",
    "    feat_cols_used = select_feat_cols(feat_cols_all, exclude_gj=exclude_gj, exclude_axis=exclude_axis)\n",
    "    X = df_win[feat_cols_used].to_numpy(float)\n",
    "    print(\"Selecteed Features for Training\",len(feat_cols_used))\n",
    "    y = df_win[\"label\"].to_numpy(int)\n",
    "\n",
    "    # ------- choose grouping key -------\n",
    "    groups = None\n",
    "    group_name = None\n",
    "    if group_by_file and \"file\" in df_win.columns and df_win[\"file\"].nunique() >= 2:\n",
    "        groups = df_win[\"file\"].astype(str).to_numpy()\n",
    "        group_name = \"file\"\n",
    "        print(\"data grouped\")\n",
    "    elif \"seg_uid\" in df_win.columns and df_win[\"seg_uid\"].nunique() >= 2:\n",
    "        groups = df_win[\"seg_uid\"].astype(str).to_numpy()\n",
    "        group_name = \"seg_uid\"\n",
    "\n",
    "    # ------- pipeline -------\n",
    "    steps = [(\"scaler\", StandardScaler())]\n",
    "    if use_pca:\n",
    "        steps.append((\"pca\", PCA(n_components=float(pca_var), svd_solver=\"full\",\n",
    "                                 whiten=False, random_state=random_state)))\n",
    "    try:\n",
    "        gval = float(gamma_val)\n",
    "    except Exception:\n",
    "        gval = gamma_val  # \"scale\"/\"auto\"\n",
    "    clf = SVC(kernel=kernel,\n",
    "              C=float(C_val),\n",
    "              gamma=gval if kernel == \"rbf\" else \"scale\",\n",
    "              class_weight=\"balanced\",\n",
    "              probability=True,\n",
    "              random_state=random_state)\n",
    "    steps.append((\"clf\", clf))\n",
    "    pipe = Pipeline(steps)\n",
    "\n",
    "    # ------- cross-validation (no leakage) -------\n",
    "    scoring = [\"accuracy\",\"balanced_accuracy\",\"precision_macro\",\"recall_macro\",\"f1_macro\",\"f1_weighted\"]\n",
    "    if group_name is not None:\n",
    "        print(\"cv grouped\")\n",
    "        cv = GroupKFold(n_splits=max(2, int(cv_folds)))\n",
    "        scores = cross_validate(pipe, X, y, groups=groups, cv=cv,\n",
    "                                scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "    else:\n",
    "        print(\"cv not grouped\")\n",
    "        # no grouping available → vanilla CV（注意这时只是报告用，holdout 再做防泄露切分）\n",
    "        scores = cross_validate(pipe, X, y, cv=max(2, int(cv_folds)),\n",
    "                                scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "\n",
    "    # ------- holdout split (strict) -------\n",
    "    if group_name is not None:\n",
    "        print(\"holdout grouped\")\n",
    "        if test_ratio!=0:\n",
    "            gss = GroupShuffleSplit(n_splits=1, train_size=1.0-float(test_ratio), random_state=random_state)\n",
    "            idx_tr, idx_va = next(gss.split(X, y, groups))\n",
    "            X_tr, X_va, y_tr, y_va = X[idx_tr], X[idx_va], y[idx_tr], y[idx_va]\n",
    "        else:\n",
    "            gss = GroupShuffleSplit(n_splits=1, train_size=1.0-float(0.1), random_state=random_state)\n",
    "            idx_tr, idx_va = next(gss.split(X, y, groups))\n",
    "            X_tr, X_va, y_tr, y_va = X, X[idx_va], y, y[idx_va]\n",
    "            \n",
    "    else:\n",
    "        print(\"holdout not grouped\")\n",
    "        # time-ordered split + a small gap to reduce overlap leakage\n",
    "        if \"t_start\" in df_win.columns and \"t_end\" in df_win.columns:\n",
    "            t_mid = 0.5*(df_win[\"t_start\"].to_numpy(float) + df_win[\"t_end\"].to_numpy(float))\n",
    "        else:\n",
    "            # fallback: use row order\n",
    "            t_mid = np.arange(len(df_win), dtype=float)\n",
    "        order = np.argsort(t_mid)\n",
    "        n = len(order)\n",
    "        n_tr = int((1.0 - float(test_ratio)) * n)\n",
    "        gap = max(1, int(0.01 * n))  # 1% gap at the split point\n",
    "        idx_tr = order[:max(0, n_tr - gap)]\n",
    "        idx_va = order[min(n, n_tr + gap):]\n",
    "        # last resort: if something went wrong, do stratified random split\n",
    "        if idx_tr.size == 0 or idx_va.size == 0:\n",
    "            idx_tr, idx_va = train_test_split(\n",
    "                np.arange(n),\n",
    "                test_size=float(test_ratio),\n",
    "                random_state=random_state,\n",
    "                stratify=y if len(np.unique(y)) > 1 else None\n",
    "            )\n",
    "        X_tr, X_va, y_tr, y_va = X[idx_tr], X[idx_va], y[idx_tr], y[idx_va]\n",
    "\n",
    "    # ------- fit & evaluate -------\n",
    "    \n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_pred = pipe.predict(X_va)\n",
    "\n",
    "    acc_h  = accuracy_score(y_va, y_pred)\n",
    "    balc_h = balanced_accuracy_score(y_va, y_pred)\n",
    "    prec_h, rec_h, f1_h, _ = precision_recall_fscore_support(y_va, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1w_h = precision_recall_fscore_support(y_va, y_pred, average=\"weighted\", zero_division=0)[2]\n",
    "\n",
    "    report = classification_report(y_va, y_pred, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(y_va, y_pred, labels=sorted(np.unique(y)))\n",
    "\n",
    "    # ------- save model (with reproducibility meta) -------\n",
    "    tag = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = MODEL_DIR / f\"svm_motion_{kernel}_{tag}.pkl\"\n",
    "    joblib.dump(dict(\n",
    "        pipeline=pipe,\n",
    "        meta=dict(\n",
    "            tag=tag, kernel=kernel, C=float(C_val), gamma=str(gamma_val),\n",
    "            pca=bool(use_pca), pca_var=float(pca_var),\n",
    "            cv_folds=int(cv_folds), test_ratio=float(test_ratio),\n",
    "            group_by=(group_name or \"none\"),\n",
    "            exclude_gj=bool(exclude_gj),\n",
    "            feat_cols_used=feat_cols_used\n",
    "        )\n",
    "    ), model_path)\n",
    "\n",
    "    cv_summary = {k.replace(\"test_\",\"\"): (float(np.mean(v)), float(np.std(v)))\n",
    "                  for k, v in scores.items() if k.startswith(\"test_\")}\n",
    "\n",
    "    return pipe, dict(\n",
    "        cv=cv_summary,\n",
    "        holdout=dict(acc=acc_h, balc=balc_h, prec=prec_h, rec=rec_h, f1=f1_h, f1w=f1w_h, n=int(len(y_va))),\n",
    "        report=report, cm=cm, model_path=str(model_path), classes=sorted(np.unique(y))\n",
    "    )\n",
    "\n",
    "# Confusion matrix rendering (row-% color + text)\n",
    "\n",
    "def confusion_figure(cm: np.ndarray, classes: List[int]):\n",
    "    cm = np.asarray(cm, int)\n",
    "    rowsum = cm.sum(axis=1, keepdims=True)\n",
    "    rowsum[rowsum==0] = 1\n",
    "    pct = cm / rowsum\n",
    "    cls_names = [f\"{c}:{LABEL_MAP.get(c,'cls')}\" for c in classes]\n",
    "    text = [[f\"{100*pct[i,j]:.1f}%(n={cm[i,j]})\" for j in range(cm.shape[1])] for i in range(cm.shape[0])]\n",
    "    fig = go.Figure(data=go.Heatmap(z=pct, x=cls_names, y=cls_names, colorscale=\"Blues\",\n",
    "                                    zmin=0.0, zmax=1.0, showscale=True, text=text, texttemplate=\"%{text}\",\n",
    "                                    hovertemplate=\"True=%{y}<br>Pred=%{x}<br>%{text}<extra></extra>\"))\n",
    "    fig.update_layout(height=360, margin=dict(l=60,r=40,t=50,b=50), paper_bgcolor=\"white\", plot_bgcolor=\"white\",\n",
    "                      title=\"Confusion Matrix (row-normalized %)\")\n",
    "    fig.update_xaxes(title_text=\"Predicted\")\n",
    "    fig.update_yaxes(title_text=\"True\")\n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"ddl-train-win\", \"options\"),\n",
    "    Output(\"ddl-train-win\", \"value\"),\n",
    "    Input(\"btn-scan-win\", \"n_clicks\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def scan_train_window(_):\n",
    "    files = sorted(DIR_TRAIN_WIN.glob(\"*.csv\"),\n",
    "                   key=lambda p: p.stat().st_mtime,\n",
    "                   reverse=True)\n",
    "    opts = [{\"label\": f.name, \"value\": str(f)} for f in files]\n",
    "    default = str(files[0]) if files else None\n",
    "    return opts, default\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"ddl-holdout-files\", \"options\"),\n",
    "    Input(\"btn-refresh-trainval\", \"n_clicks\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def refresh_trainval_list(_):\n",
    "    files = sorted(DIR_TRAIN_VAL.glob(\"*.csv\"))\n",
    "    return [{\"label\": f.name, \"value\": str(f)} for f in files]\n",
    "\n",
    "\n",
    "# Train callback\n",
    "@app.callback(\n",
    "    Output(\"txt-train-status\", \"children\"),\n",
    "    Output(\"div-train-metrics\", \"children\"),\n",
    "    Output(\"fig-cm\", \"figure\"),\n",
    "    Output(\"fig-f1\", \"figure\"),\n",
    "    Input(\"btn-train\", \"n_clicks\"),\n",
    "    State(\"ddl-train-win\", \"value\"),\n",
    "    State(\"ri-kernel\", \"value\"),\n",
    "    State(\"input-C\", \"value\"),\n",
    "    State(\"input-gamma\", \"value\"),\n",
    "    State(\"chk-pca\", \"value\"),\n",
    "    State(\"sl-pca-var\", \"value\"),\n",
    "    State(\"input-cv\", \"value\"),\n",
    "    State(\"input-test\", \"value\"),\n",
    "    State(\"chk-group-file\", \"value\"),\n",
    "    State(\"chk-excl-gj\",\"value\"),\n",
    "    State(\"chk-excl-axis\",\"value\"),\n",
    "    State(\"ddl-holdout-files\", \"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def train_now(nc, ds_file, kernel, C_val, gamma_val, chk_pca, pca_var, cv_folds, test_ratio, chk_group, chk_excl_gj, chk_excl_axis, holdout_paths):\n",
    "    if not ds_file:\n",
    "        return \"Select a train_window CSV first.\", \"\", go.Figure(), go.Figure()\n",
    "\n",
    "    df = pd.read_csv(ds_file)\n",
    "    print(\"Unique files:\", df['file'].nunique())\n",
    "    if 'seg_uid' in df.columns:\n",
    "        print(\"Unique segments:\", df['seg_uid'].nunique())\n",
    "\n",
    "    use_pca = (chk_pca is not None and \"use\" in chk_pca)\n",
    "    group_by_file = (chk_group is not None and \"group\" in chk_group)\n",
    "    exclude_gj = (chk_excl_gj is not None and \"excl\" in chk_excl_gj)\n",
    "    print(\"GJ by training\",exclude_gj)\n",
    "    exclude_axis = (\"excl_axis\" in (chk_excl_axis or []))\n",
    "    pipe, info = train_pipeline_quick(df, kernel, float(C_val), gamma_val,\n",
    "                                        use_pca, float(pca_var), int(cv_folds), float(test_ratio), group_by_file, exclude_gj, exclude_axis)\n",
    "    if not holdout_paths:\n",
    "        print( \"❗Select holdout CSV(s) from /train_val.\")\n",
    "        return \"❗Select holdout CSV(s) from /train_val.\", \"\", go.Figure(), go.Figure()\n",
    "    \n",
    "    # 读入 holdout 并对齐训练用特征\n",
    "    if isinstance(holdout_paths, str):\n",
    "        holdout_paths = [holdout_paths]\n",
    "    dfs_ho = []\n",
    "    for pth in holdout_paths:\n",
    "        try:\n",
    "            print(\"Holdout read success\")\n",
    "            dfs_ho.append(pd.read_csv(pth))\n",
    "        except Exception as e:\n",
    "            print(\"Holdout read failed:\", pth, e)\n",
    "    if not dfs_ho:\n",
    "        return \"❗Empty holdout set.\", \"\", go.Figure(), go.Figure()\n",
    "    df_ho = pd.concat(dfs_ho, axis=0, ignore_index=True)\n",
    "\n",
    "    feat_cols_used = info.get(\"feat_cols_used\", None)\n",
    "    if feat_cols_used is None:\n",
    "        # 与 train_pipeline_quick 内部保持一致的选择\n",
    "        X_all, y_all, _, feat_cols_all = build_Xy_from_df(df)\n",
    "        feat_cols_used = select_feat_cols(feat_cols_all, exclude_gj=exclude_gj, exclude_axis=exclude_axis)\n",
    "\n",
    "    # 对齐列顺序（缺失列补0）\n",
    "    X_ho = df_ho.reindex(columns=feat_cols_used, fill_value=0.0).to_numpy(float)\n",
    "    y_ho = df_ho[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "    y_pred = pipe.predict(X_ho)\n",
    "    from sklearn.metrics import (accuracy_score, balanced_accuracy_score,\n",
    "                                    precision_recall_fscore_support,\n",
    "                                    classification_report, confusion_matrix)\n",
    "    acc_h  = float(accuracy_score(y_ho, y_pred))\n",
    "    balc_h = float(balanced_accuracy_score(y_ho, y_pred))\n",
    "    prec_h, rec_h, f1_h, _ = precision_recall_fscore_support(y_ho, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1w_h = precision_recall_fscore_support(y_ho, y_pred, average=\"weighted\", zero_division=0)[2]\n",
    "    report = classification_report(y_ho, y_pred, output_dict=True, zero_division=0)\n",
    "    classes = sorted(np.unique(np.concatenate([y_ho, y_pred])))\n",
    "    cm = confusion_matrix(y_ho, y_pred, labels=classes)\n",
    "\n",
    "    # —— 组织文本与图：CV（来自 info） + Holdout（外部） ——\n",
    "    cv = info['cv']\n",
    "    def fmt(mu_sd): mu, sd = mu_sd; return f\"{mu:.3f}±{sd:.3f}\"\n",
    "    txt = f\"Model saved: {info['model_path']}\"\n",
    "\n",
    "    metrics_div = html.Div([\n",
    "        html.Div([html.Strong(\"Cross-Validation (mean±std): \"),\n",
    "                    html.Span(f\"Acc {fmt(cv['accuracy'])} | BalAcc {fmt(cv['balanced_accuracy'])} | \"\n",
    "                            f\"Prec_macro {fmt(cv['precision_macro'])} | Rec_macro {fmt(cv['recall_macro'])} | \"\n",
    "                            f\"F1_macro {fmt(cv['f1_macro'])} | F1_weighted {fmt(cv['f1_weighted'])}\")]),\n",
    "        html.Div([html.Strong(\"Holdout (external from /train_val): \"),\n",
    "                    html.Span(f\"Acc {acc_h:.3f} | BalAcc {balc_h:.3f} | \"\n",
    "                            f\"Prec_macro {prec_h:.3f} | Rec_macro {rec_h:.3f} | \"\n",
    "                            f\"F1_macro {f1_h:.3f} | F1_weighted {f1w_h:.3f} | \"\n",
    "                            f\"Samples {len(y_ho)}\")])\n",
    "    ])\n",
    "\n",
    "    # 混淆矩阵\n",
    "    fig_cm = confusion_figure(cm, classes)\n",
    "\n",
    "    # per-class F1\n",
    "    labels_txt = [f\"{c}:{LABEL_MAP.get(c,'cls')}\" for c in classes]\n",
    "    f1_vals = [float(report.get(str(c), {}).get(\"f1-score\", 0.0)) for c in classes]\n",
    "    fig_f1 = go.Figure(data=go.Bar(x=labels_txt, y=f1_vals, name=\"F1\"))\n",
    "    fig_f1.update_layout(title=\"Per-class F1 — external holdout\",\n",
    "                            yaxis_title=\"F1\", xaxis_title=\"Class\",\n",
    "                            height=320, paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "\n",
    "    return txt, metrics_div, fig_cm, fig_f1\n",
    "\n",
    "\n",
    "    \n",
    "@app.callback(\n",
    "    Output(\"fig-infer-preview\",\"figure\"),\n",
    "    Input(\"ddl-files\",\"value\"),\n",
    "    Input(\"rs-seg\",\"value\"),\n",
    "    State(\"input-ppg-col\",\"value\"),\n",
    "    State(\"input-fs\",\"value\"),\n",
    "    State(\"input-win\",\"value\"),\n",
    "    State(\"input-hop\",\"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def infer_preview(file_path, seg, ppg_col, fs, win_sec, hop_sec):\n",
    "    fig_empty = go.Figure().update_layout(title=\"No model / no segment\", height=160,\n",
    "                                          paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "    try:\n",
    "        if not file_path or not seg:\n",
    "            return fig_empty\n",
    "        # 取最新模型\n",
    "        models = sorted(MODEL_DIR.glob(\"svm_motion_*.pkl\"),\n",
    "                        key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if not models:\n",
    "            return go.Figure().update_layout(title=\"No trained model found\", height=160,\n",
    "                                             paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "\n",
    "        M = joblib.load(models[0])\n",
    "        pipe = M[\"pipeline\"]\n",
    "        feat_used = M[\"meta\"][\"feat_cols_used\"]\n",
    "        print(\"Selected Features for Predicting\",len(feat_used))\n",
    "        # 读入选定文件并截取区间\n",
    "        fs = float(fs or FS_DEFAULT)\n",
    "        df = pd.read_csv(file_path)\n",
    "        if ppg_col not in df.columns:\n",
    "            ppg_col = \"IR\" if \"IR\" in df.columns else df.columns[0]\n",
    "        t_all = np.arange(len(df))/fs\n",
    "        mask = (t_all >= seg[0]) & (t_all <= seg[1])\n",
    "        df_seg = df.loc[mask].reset_index(drop=True)\n",
    "        ppg = df_seg[ppg_col].to_numpy(float)\n",
    "        N = len(ppg)\n",
    "        if N < int(max(1.0, float(win_sec))*fs):\n",
    "            return go.Figure().update_layout(title=\"Window too short\", height=160,\n",
    "                                             paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "\n",
    "        # IMU 预处理（与训练一致）\n",
    "        imu_out = imu_preprocess_with_kf(df_seg, fs=fs)\n",
    "        acc_g = imu_out['acc_raw']\n",
    "        gyro_d = imu_out['gyro_raw']\n",
    "        acc = imu_out['acc']\n",
    "        gyro = imu_out['gyro']\n",
    "        acc_lp = imu_out['acc_f'] \n",
    "        gyro_lp = imu_out['gyr_f'] \n",
    "        jerk = imu_out['jerk'] \n",
    "        roll=imu_out['roll'] \n",
    "        pitch=imu_out['pitch'] \n",
    "        bg = imu_out['gyro_bias']\n",
    "        g_body=imu_out['g_body']\n",
    "        g_dir = imu_out['g_dir'] \n",
    "        a_dyn= imu_out['a_dyn']\n",
    "        acc_mag = imu_out['AccMag'] \n",
    "        gyro_mag = imu_out['GyroMag'] \n",
    "        jerk_mag = imu_out['JerkMag']\n",
    "        rows, names = [], None\n",
    "        for s, e in make_windows(N, fs, float(win_sec), float(hop_sec)):\n",
    "            ppg_win = preprocess_ppg_min(ppg, fs=fs, hp_cut=0.2, mains=50)\n",
    "            ax_win = a_dyn[s:e, 0]; ay_win = a_dyn[s:e, 1]; az_win = a_dyn[s:e, 2]\n",
    "            accmag_win = acc_mag[s:e]\n",
    "            gyromag_win = gyro_mag[s:e] #if 'GyroMag' in locals() else None\n",
    "            jerkmag_win = jerk_mag[s:e] #if 'JerkMag' in locals() else None\n",
    "            roll_win = roll[s:e]\n",
    "            pitch_win = pitch[s:e]\n",
    "            names, feats = extract_features_window(\n",
    "                fs=fs,\n",
    "                ppg_win=ppg_win,\n",
    "                a_dyn_xyz=(ax_win, ay_win, az_win),\n",
    "                accmag_win=accmag_win,\n",
    "                gyromag_win=gyromag_win,\n",
    "                jerkmag_win=jerkmag_win,\n",
    "                roll_win=roll_win, pitch_win=pitch_win,\n",
    "                psd_fmax=8.0\n",
    "            )\n",
    "            rows.append(feats)\n",
    "        if not rows:\n",
    "            return go.Figure().update_layout(title=\"No windows generated\", height=160,\n",
    "                                             paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "\n",
    "        X_all = pd.DataFrame(rows, columns=names)\n",
    "        # 对齐训练时的特征列\n",
    "        X = X_all.reindex(columns=feat_used, fill_value=0.0).to_numpy(float)\n",
    "        #print(X)\n",
    "        print(\"Shape of Data Features DF for predicting Segment\",np.shape(X))\n",
    "        yhat = pipe.predict(X).astype(int)\n",
    "\n",
    "        # 生成离散色热力图（无渐变条）\n",
    "        ts = seg[0] + (np.arange(len(yhat))*float(hop_sec) + float(win_sec)/2.0)\n",
    "        z = yhat[np.newaxis, :]\n",
    "        cs = discrete_colorscale_from_map(LABEL_COLORS)\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=z, x=ts, y=[\"State\"], colorscale=cs, showscale=False,\n",
    "            zmin=min(LABEL_COLORS.keys()), zmax=max(LABEL_COLORS.keys())\n",
    "        ))\n",
    "        # 图例（离散标注）\n",
    "        for k, color in LABEL_COLORS.items():\n",
    "            fig.add_trace(go.Scatter(x=[None], y=[None], mode=\"markers\",\n",
    "                                     marker=dict(color=color, size=10),\n",
    "                                     name=f\"{k}:{LABEL_MAP[k]}\"))\n",
    "        fig.update_layout(height=140, margin=dict(l=50, r=30, t=40, b=30),\n",
    "                          title=\"Predicted Motion (discrete)\",\n",
    "                          paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        return go.Figure().update_layout(title=f\"Inference error: {e}\", height=160,\n",
    "                                         paper_bgcolor=\"white\", plot_bgcolor=\"white\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2da16949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8051/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7c3b15970210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique files: 60\n",
      "GJ by training True\n",
      "GY: True\n",
      "Selecteed Features for Training 88\n",
      "data grouped\n",
      "cv grouped\n",
      "holdout grouped\n",
      "Holdout read success\n",
      "GY: True\n",
      "Unique files: 60\n",
      "GJ by training True\n",
      "GY: True\n",
      "Selecteed Features for Training 88\n",
      "data grouped\n",
      "cv grouped\n",
      "holdout grouped\n",
      "Holdout read success\n",
      "GY: True\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Entrypoint\n",
    "# ========================\n",
    "\n",
    "def main():\n",
    "    app.run(debug=True, port=8051)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b8a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4682b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841b538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9686ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
